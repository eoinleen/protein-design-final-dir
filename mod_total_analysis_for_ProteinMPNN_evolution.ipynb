{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Igu8xeVHwjf3ZWC6U-prJqd4Ms9PSl07",
      "authorship_tag": "ABX9TyOaxvV4BbKgB5BtPvsmC+Zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eoinleen/protein-design-final-dir/blob/main/mod_total_analysis_for_ProteinMPNN_evolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nPy-faEiG1H",
        "outputId": "9ee887a1-9548-40dc-b096-75c9f06b2390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/270.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.1/270.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for freesasa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "No AF2 scores file found - will generate structure analysis only\n",
            "\n",
            "Starting analysis...\n",
            "Processing PDB files from: /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders\n",
            "Processing 13 PDB files...\n",
            "Processing file 1/13: 1_bind_0_dldesign_9965_af2pred.pdb\n",
            "Processing file 2/13: 1_bind_0_dldesign_2056_af2pred.pdb\n",
            "Processing file 3/13: 1_bind_0_dldesign_5160_af2pred.pdb\n",
            "Processing file 4/13: 1_bind_0_dldesign_7384_af2pred.pdb\n",
            "Processing file 5/13: 1_bind_0_dldesign_2304_af2pred.pdb\n",
            "Processing file 6/13: 1_bind_0_dldesign_7578_af2pred.pdb\n",
            "Processing file 7/13: 1_bind_0_dldesign_8480_af2pred.pdb\n",
            "Processing file 8/13: 1_bind_0_dldesign_2777_af2pred.pdb\n",
            "Processing file 9/13: 1_bind_0_dldesign_1829_af2pred.pdb\n",
            "Processing file 10/13: 1_bind_0_dldesign_5024_af2pred.pdb\n",
            "Processing file 11/13: 1_bind_0_dldesign_2795_af2pred.pdb\n",
            "Processing file 12/13: 1_bind_0_dldesign_1708_af2pred.pdb\n",
            "Processing file 13/13: 1_bind_0design0_af2pred.pdb\n",
            "Warning: /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders/1_bind_0design0_af2pred.pdb has fewer than 2 chains\n",
            "Warning: Unrecognized filename format: 1_bind_0design0_af2pred\n",
            "Saved structure analysis to /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders/0_top_binders_250303_structure.csv\n",
            "Saved structural parameter visualization to /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders/0_top_binders_250303_structural_params.png\n",
            "Saved correlation matrix to /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders/0_top_binders_250303_correlation.png\n",
            "Saved normalized parameters comparison to /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders/0_top_binders_250303_normalized_params.png\n",
            "Saved interactive visualization to /content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders/0_top_binders_250303_interactive.html\n",
            "Error creating PowerPoint summary: assigned value must be type RGBColor\n",
            "\n",
            "Summary of results:\n",
            "Total structures analyzed: 13\n",
            "Average buried surface area: 823.43 Å²\n",
            "Average hydrogen bonds: 0.00\n",
            "Average hydrophobic contacts: 19.69\n",
            "Average salt bridges: 2.77\n",
            "\n",
            "Top structures by buried surface area:\n",
            "Design 2056 - 1_bind_0_dldesign_2056_af2pred.pdb: 922.25 Å²\n",
            "Design 8480 - 1_bind_0_dldesign_8480_af2pred.pdb: 920.07 Å²\n",
            "Design 7384 - 1_bind_0_dldesign_7384_af2pred.pdb: 909.95 Å²\n",
            "\n",
            "Top structures by hydrogen bonds:\n",
            "Design 1496 - 1_bind_0design0_af2pred.pdb: 0 bonds\n",
            "Design 1708 - 1_bind_0_dldesign_1708_af2pred.pdb: 0 bonds\n",
            "Design 1829 - 1_bind_0_dldesign_1829_af2pred.pdb: 0 bonds\n",
            "\n",
            "Analysis completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-1-e2c64f869283>\", line 538, in create_pptx_summary\n",
            "    cell.fill.fore_color.rgb = (200, 200, 200)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pptx/dml/color.py\", line 62, in rgb\n",
            "    raise ValueError(\"assigned value must be type RGBColor\")\n",
            "ValueError: assigned value must be type RGBColor\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "RFdiffusion Structure Analysis and Sequence Extraction Tool\n",
        "========================================================\n",
        "Created: January 31, 2025\n",
        "Authors: Original Analysis - Dr. Eoin Leen, University of Leeds\n",
        "         Visualization & Integration - Claude AI & Dr. Eoin Leen\n",
        "Version: 2.1\n",
        "\n",
        "Purpose:\n",
        "--------\n",
        "Combined pipeline for:\n",
        "1. Structural analysis of PDB files\n",
        "2. Interface analysis for protein-protein interactions\n",
        "3. Generation of publication-ready visualizations\n",
        "\n",
        "Input Required:\n",
        "-------------\n",
        "1. Directory containing PDB files\n",
        "\n",
        "Output Generated:\n",
        "---------------\n",
        "1. CSV file with structural analysis results\n",
        "2. Visualizations of structural parameters\n",
        "\n",
        "Analysis Parameters:\n",
        "------------------\n",
        "Structure Analysis Cutoffs:\n",
        "1. Hydrogen Bonds:\n",
        "   - Distance cutoff: O-N distance < 3.5 Å\n",
        "   - Calculated between backbone atoms only\n",
        "   - Only inter-chain H-bonds counted\n",
        "\n",
        "2. Salt Bridges:\n",
        "   - Distance cutoff: < 4.0 Å between any atoms of residue pairs\n",
        "   - Residue pairs considered:\n",
        "     * Acidic: ASP, GLU\n",
        "     * Basic: LYS, ARG, HIS\n",
        "   - Only inter-chain salt bridges counted\n",
        "\n",
        "3. Hydrophobic Contacts:\n",
        "   - Distance cutoff: < 5.0 Å between any atoms of residue pairs\n",
        "   - Hydrophobic residues considered:\n",
        "     * ALA, VAL, LEU, ILE, MET, PHE, TRP, PRO\n",
        "   - Only inter-chain contacts counted\n",
        "\n",
        "4. Buried Surface Area:\n",
        "   - Calculated using FreeSASA algorithm\n",
        "   - Uses default atomic radii from FreeSASA (based on NACCESS/RSA)\n",
        "   - Process:\n",
        "     * First calculates SASA for entire complex\n",
        "     * Then calculates SASA for each chain individually\n",
        "     * BSA = (Sum of individual chain SASAs - Complex SASA) / 2\n",
        "   - Units: Å²\n",
        "   - Inter-chain burial only (interface area)\n",
        "   - Probe radius: 1.4 Å (water molecule)\n",
        "   - Resolution: 100 points/atom (FreeSASA default)\n",
        "\n",
        "Usage:\n",
        "-----\n",
        "1. Mount Google Drive in Colab\n",
        "2. Update pdb_directory path\n",
        "3. Run script\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q biopython pandas freesasa numpy matplotlib seaborn python-pptx plotly kaleido\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from Bio import PDB\n",
        "from Bio.PDB.PDBIO import PDBIO\n",
        "from Bio.PDB.Polypeptide import is_aa\n",
        "from Bio.PDB.Structure import Structure\n",
        "import freesasa\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Cm, Pt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import traceback\n",
        "import hashlib\n",
        "\n",
        "# Custom exception for structure validation\n",
        "class StructureValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "# ===============================\n",
        "# Structure Analysis Functions\n",
        "# ===============================\n",
        "\n",
        "def validate_pdb_file(file_path: str) -> bool:\n",
        "    \"\"\"Validates if file exists and has proper PDB format.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"PDB file not found: {file_path}\")\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            first_line = f.readline()\n",
        "            if not any(marker in first_line for marker in ['HEADER', 'ATOM', 'MODEL']):\n",
        "                raise StructureValidationError(f\"Invalid PDB: {file_path}\")\n",
        "    except UnicodeDecodeError:\n",
        "        raise StructureValidationError(f\"Not a valid text file: {file_path}\")\n",
        "    return True\n",
        "\n",
        "def safe_structure_load(parser: PDB.PDBParser, file_path: str) -> Optional[Structure]:\n",
        "    \"\"\"Safely loads PDB structure with error handling.\"\"\"\n",
        "    try:\n",
        "        validate_pdb_file(file_path)\n",
        "        structure = parser.get_structure('protein', file_path)\n",
        "        if not list(structure.get_models()):\n",
        "            raise StructureValidationError(\"No models\")\n",
        "        if not list(list(structure.get_models())[0].get_chains()):\n",
        "            raise StructureValidationError(\"No chains\")\n",
        "        return structure\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def calculate_buried_surface_area(pdb_file: str) -> Tuple[Optional[float], Optional[Dict[str, float]]]:\n",
        "    \"\"\"Calculates buried surface area between chains.\"\"\"\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    structure = safe_structure_load(parser, pdb_file)\n",
        "    if not structure:\n",
        "        return None, None\n",
        "    try:\n",
        "        chains = list(structure.get_chains())\n",
        "        if len(chains) < 2:\n",
        "            print(f\"Warning: {pdb_file} has fewer than 2 chains\")\n",
        "            return None, None\n",
        "\n",
        "        combined_structure = freesasa.Structure(pdb_file)\n",
        "        result = freesasa.calc(combined_structure)\n",
        "        total_area = result.totalArea()\n",
        "\n",
        "        chain_areas = {}\n",
        "        io = PDBIO()\n",
        "        temp_files = []\n",
        "\n",
        "        for chain in chains:\n",
        "            new_structure = PDB.Structure.Structure('temp')\n",
        "            new_model = PDB.Model.Model(0)\n",
        "            new_structure.add(new_model)\n",
        "            new_model.add(chain.copy())\n",
        "\n",
        "            temp_file = f\"temp_chain_{chain.id}.pdb\"\n",
        "            temp_files.append(temp_file)\n",
        "\n",
        "            io.set_structure(new_structure)\n",
        "            io.save(temp_file)\n",
        "\n",
        "            chain_structure = freesasa.Structure(temp_file)\n",
        "            chain_result = freesasa.calc(chain_structure)\n",
        "            chain_areas[chain.id] = chain_result.totalArea()\n",
        "\n",
        "        for temp_file in temp_files:\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "        total_individual_area = sum(chain_areas.values())\n",
        "        buried_surface_area = abs(total_individual_area - total_area) / 2\n",
        "        return buried_surface_area, chain_areas\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BSA for {pdb_file}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def calculate_hydrogen_bonds(structure: Structure) -> int:\n",
        "    \"\"\"Calculates number of hydrogen bonds between chains.\"\"\"\n",
        "    try:\n",
        "        h_bonds = 0\n",
        "        for chain1 in structure.get_chains():\n",
        "            for chain2 in structure.get_chains():\n",
        "                if chain1.id >= chain2.id:\n",
        "                    continue\n",
        "                for res1 in chain1.get_residues():\n",
        "                    if not is_aa(res1):\n",
        "                        continue\n",
        "                    for res2 in chain2.get_residues():\n",
        "                        if not is_aa(res2):\n",
        "                            continue\n",
        "                        if 'O' in res1 and 'N' in res2:\n",
        "                            distance = res1['O'] - res2['N']\n",
        "                            if distance < 3.5:\n",
        "                                h_bonds += 1\n",
        "        return h_bonds\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating H-bonds: {str(e)}\")\n",
        "        return 0\n",
        "def calculate_hydrophobic_contacts(structure: Structure) -> int:\n",
        "    \"\"\"\n",
        "    Calculates number of hydrophobic contacts between chains.\n",
        "    Considers residues ALA, VAL, LEU, ILE, MET, PHE, TRP, PRO.\n",
        "    Contact is counted if distance < 5.0 Å.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        hydrophobic_residues = {'ALA', 'VAL', 'LEU', 'ILE', 'MET', 'PHE', 'TRP', 'PRO'}\n",
        "        contacts = 0\n",
        "        for chain1 in structure.get_chains():\n",
        "            for chain2 in structure.get_chains():\n",
        "                if chain1.id >= chain2.id:\n",
        "                    continue\n",
        "                for res1 in chain1.get_residues():\n",
        "                    if not is_aa(res1) or res1.get_resname() not in hydrophobic_residues:\n",
        "                        continue\n",
        "                    for res2 in chain2.get_residues():\n",
        "                        if not is_aa(res2) or res2.get_resname() not in hydrophobic_residues:\n",
        "                            continue\n",
        "                        min_distance = float('inf')\n",
        "                        for atom1 in res1.get_atoms():\n",
        "                            for atom2 in res2.get_atoms():\n",
        "                                distance = atom1 - atom2\n",
        "                                min_distance = min(min_distance, distance)\n",
        "                        if min_distance < 5.0:\n",
        "                            contacts += 1\n",
        "        return contacts\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating hydrophobic contacts: {str(e)}\")\n",
        "        return 0\n",
        "\n",
        "def calculate_salt_bridges(structure: Structure) -> int:\n",
        "    \"\"\"\n",
        "    Calculates number of salt bridges between chains.\n",
        "    Salt bridge is counted between ASP/GLU and LYS/ARG/HIS if distance < 4.0 Å.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        acidic = {'ASP', 'GLU'}\n",
        "        basic = {'LYS', 'ARG', 'HIS'}\n",
        "        salt_bridges = 0\n",
        "        for chain1 in structure.get_chains():\n",
        "            for chain2 in structure.get_chains():\n",
        "                if chain1.id >= chain2.id:\n",
        "                    continue\n",
        "                for res1 in chain1.get_residues():\n",
        "                    if not is_aa(res1):\n",
        "                        continue\n",
        "                    res1_name = res1.get_resname()\n",
        "                    for res2 in chain2.get_residues():\n",
        "                        if not is_aa(res2):\n",
        "                            continue\n",
        "                        res2_name = res2.get_resname()\n",
        "                        if ((res1_name in acidic and res2_name in basic) or\n",
        "                            (res1_name in basic and res2_name in acidic)):\n",
        "                            min_distance = float('inf')\n",
        "                            for atom1 in res1.get_atoms():\n",
        "                                for atom2 in res2.get_atoms():\n",
        "                                    distance = atom1 - atom2\n",
        "                                    min_distance = min(min_distance, distance)\n",
        "                            if min_distance < 4.0:\n",
        "                                salt_bridges += 1\n",
        "        return salt_bridges\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating salt bridges: {str(e)}\")\n",
        "        return 0\n",
        "\n",
        "def save_results_as_df(results: List[Dict[str, Any]], output_file: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts analysis results to DataFrame and saves to CSV.\n",
        "    Extracts design identifiers from filenames with flexible pattern matching.\n",
        "    \"\"\"\n",
        "    analysis_data = []\n",
        "    for result in results:\n",
        "        filename = result['file_name'].replace('.pdb', '')\n",
        "        try:\n",
        "            # Handle new filename format (1_bind_0_dldesign_9965_af2pred)\n",
        "            if 'dldesign_' in filename:\n",
        "                # Extract design number (9965 from example)\n",
        "                design_num = int(filename.split('dldesign_')[1].split('_')[0])\n",
        "                # Use 1 as default variant number or extract from filename if available\n",
        "                variant_num = 1\n",
        "\n",
        "                # If there's an actual variant indicator in the filename, use that instead\n",
        "                if '_af2pred' in filename:\n",
        "                    pred_parts = filename.split('_af2pred')\n",
        "                    # If there's a variant number before af2pred\n",
        "                    if len(pred_parts) > 1 and pred_parts[1].startswith('_v'):\n",
        "                        try:\n",
        "                            variant_num = int(pred_parts[1].strip('_v'))\n",
        "                        except ValueError:\n",
        "                            pass\n",
        "            # Handle original filename format (design123_n45)\n",
        "            elif 'design' in filename and '_n' in filename:\n",
        "                design_num = int(filename.split('design')[1].split('_')[0])\n",
        "                variant_num = int(filename.split('_n')[1])\n",
        "            else:\n",
        "                # Fallback for unrecognized format\n",
        "                print(f\"Warning: Unrecognized filename format: {filename}\")\n",
        "                # Use filename hash as a unique identifier\n",
        "                design_num = int(hashlib.md5(filename.encode()).hexdigest(), 16) % 10000\n",
        "                variant_num = 1\n",
        "\n",
        "            analysis_data.append({\n",
        "                'design': design_num,\n",
        "                'n': variant_num,\n",
        "                'file_name': result['file_name'],  # Keep original filename for reference\n",
        "                'buried_surface_area': result['buried_surface_area'] if result['buried_surface_area'] else 0,\n",
        "                'hydrogen_bonds': result['hydrogen_bonds'],\n",
        "                'hydrophobic_contacts': result['hydrophobic_contacts'],\n",
        "                'salt_bridges': result['salt_bridges']\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing filename {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not analysis_data:\n",
        "        print(\"Warning: No valid data extracted from filenames.\")\n",
        "        # Create empty dataframe with required columns to prevent errors\n",
        "        return pd.DataFrame(columns=['design', 'n', 'file_name', 'buried_surface_area',\n",
        "                                     'hydrogen_bonds', 'hydrophobic_contacts',\n",
        "                                     'salt_bridges'])\n",
        "\n",
        "    df = pd.DataFrame(analysis_data)\n",
        "    df = df.sort_values(['design', 'n']).reset_index(drop=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Saved structure analysis to {output_file}\")\n",
        "    return df\n",
        "\n",
        "def merge_with_af2_scores(structure_df: pd.DataFrame, af2_scores_file: str) -> pd.DataFrame:\n",
        "    \"\"\"Merges structural analysis results with AF2 scores.\"\"\"\n",
        "    try:\n",
        "        af2_df = pd.read_csv(af2_scores_file)\n",
        "        merged_df = pd.merge(af2_df, structure_df, on=['design', 'n'], how='left')\n",
        "        merged_df = merged_df.sort_values(['design', 'n']).reset_index(drop=True)\n",
        "        return merged_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error merging with AF2 scores: {str(e)}\")\n",
        "        return structure_df\n",
        "# ===============================\n",
        "# Visualization Functions\n",
        "# ===============================\n",
        "\n",
        "def create_structure_visualization(df: pd.DataFrame, output_dir: str, timestamp: str):\n",
        "    \"\"\"\n",
        "    Creates structural parameter visualizations.\n",
        "    \"\"\"\n",
        "    output_basename = os.path.basename(output_dir)\n",
        "\n",
        "    # Create a figure with subplots for each structural parameter\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Define parameters and titles\n",
        "    params = ['buried_surface_area', 'hydrogen_bonds',\n",
        "              'hydrophobic_contacts', 'salt_bridges']\n",
        "    titles = ['Buried Surface Area (Å²)', '# of Hydrogen Bonds',\n",
        "              '# of Hydrophobic Contacts', '# of Salt Bridges']\n",
        "\n",
        "    # Plot each parameter\n",
        "    for i, (param, title) in enumerate(zip(params, titles)):\n",
        "        ax = axes[i]\n",
        "        sns.barplot(x=df['design'], y=df[param], ax=ax)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('Design Number')\n",
        "        if i == 0 or i == 2:  # Left side plots\n",
        "            ax.set_ylabel(title)\n",
        "\n",
        "        # Add data labels on top of bars\n",
        "        for p in ax.patches:\n",
        "            ax.annotate(f\"{p.get_height():.1f}\",\n",
        "                      (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                      ha='center', va='center', fontsize=9,\n",
        "                      xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(output_dir, f\"{output_basename}_{timestamp}_structural_params.png\")\n",
        "    plt.savefig(plot_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved structural parameter visualization to {plot_path}\")\n",
        "\n",
        "    # Create correlation matrix visualization\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    corr_params = ['buried_surface_area', 'hydrogen_bonds',\n",
        "                   'hydrophobic_contacts', 'salt_bridges']\n",
        "\n",
        "    # Create a subset dataframe for correlation analysis\n",
        "    corr_df = df[corr_params]\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = corr_df.corr()\n",
        "\n",
        "    # Plot heatmap\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1,\n",
        "                linewidths=0.5, square=True)\n",
        "    plt.title('Correlation Matrix of Structural Parameters')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    corr_path = os.path.join(output_dir, f\"{output_basename}_{timestamp}_correlation.png\")\n",
        "    plt.savefig(corr_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved correlation matrix to {corr_path}\")\n",
        "\n",
        "    # Create a figure showing all parameters for each design\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Normalize data for fair comparison\n",
        "    norm_df = df[params].copy()\n",
        "    for param in params:\n",
        "        max_val = norm_df[param].max()\n",
        "        if max_val > 0:  # Avoid division by zero\n",
        "            norm_df[param] = norm_df[param] / max_val\n",
        "\n",
        "    # Add design column\n",
        "    norm_df['design'] = df['design']\n",
        "\n",
        "    # Reshape data for grouped bar plot\n",
        "    plot_df = pd.melt(norm_df, id_vars=['design'],\n",
        "                     value_vars=params,\n",
        "                     var_name='Parameter', value_name='Normalized Value')\n",
        "\n",
        "    # Create grouped bar plot\n",
        "    sns.barplot(x='design', y='Normalized Value', hue='Parameter', data=plot_df)\n",
        "    plt.title('Normalized Structural Parameters by Design')\n",
        "    plt.ylabel('Normalized Value')\n",
        "    plt.xlabel('Design Number')\n",
        "    plt.legend(title='Parameter')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    norm_path = os.path.join(output_dir, f\"{output_basename}_{timestamp}_normalized_params.png\")\n",
        "    plt.savefig(norm_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved normalized parameters comparison to {norm_path}\")\n",
        "\n",
        "    # Create an interactive HTML visualization using Plotly\n",
        "    try:\n",
        "        # Create a figure with subplots\n",
        "        fig = make_subplots(rows=2, cols=2,\n",
        "                           subplot_titles=titles,\n",
        "                           vertical_spacing=0.1,\n",
        "                           horizontal_spacing=0.05)\n",
        "\n",
        "        # Add traces for each parameter\n",
        "        for i, param in enumerate(params):\n",
        "            row, col = i // 2 + 1, i % 2 + 1\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(x=df['design'].astype(str),\n",
        "                      y=df[param],\n",
        "                      name=titles[i],\n",
        "                      text=df[param].round(1),\n",
        "                      textposition='auto'),\n",
        "                row=row, col=col\n",
        "            )\n",
        "\n",
        "            # Update axes labels\n",
        "            fig.update_xaxes(title_text=\"Design Number\", row=row, col=col)\n",
        "            if col == 1:  # Left side plots\n",
        "                fig.update_yaxes(title_text=titles[i], row=row, col=col)\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title='Structural Parameters Analysis',\n",
        "            height=800,\n",
        "            width=1200,\n",
        "            showlegend=False,\n",
        "            template='plotly_white'\n",
        "        )\n",
        "\n",
        "        # Save as HTML file\n",
        "        html_path = os.path.join(output_dir, f\"{output_basename}_{timestamp}_interactive.html\")\n",
        "        fig.write_html(html_path)\n",
        "        print(f\"Saved interactive visualization to {html_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating interactive visualization: {str(e)}\")\n",
        "\n",
        "def create_pptx_summary(df: pd.DataFrame, output_dir: str, timestamp: str):\n",
        "    \"\"\"\n",
        "    Creates a simple PowerPoint summary of the analysis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        output_basename = os.path.basename(output_dir)\n",
        "\n",
        "        # Initialize presentation\n",
        "        prs = Presentation()\n",
        "        prs.slide_width = Cm(25.4)\n",
        "        prs.slide_height = Cm(19.05)\n",
        "\n",
        "        # Add title slide\n",
        "        title_slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
        "        title = title_slide.shapes.title\n",
        "        subtitle = title_slide.placeholders[1]\n",
        "        title.text = \"Protein-Protein Interface Analysis\"\n",
        "        subtitle.text = f\"Generated: {time.strftime('%B %d, %Y')}\"\n",
        "\n",
        "        # Add summary slide\n",
        "        summary_slide = prs.slides.add_slide(prs.slide_layouts[5])\n",
        "        title = summary_slide.shapes.title\n",
        "        title.text = \"Analysis Summary\"\n",
        "\n",
        "        # Create a table for summary statistics\n",
        "        left = Cm(2)\n",
        "        top = Cm(4)\n",
        "        width = Cm(20)\n",
        "        height = Cm(10)\n",
        "\n",
        "        # Get summary statistics\n",
        "        stats = {\n",
        "            'Total structures analyzed': len(df),\n",
        "            'Average buried surface area': f\"{df['buried_surface_area'].mean():.2f} Å²\",\n",
        "            'Max buried surface area': f\"{df['buried_surface_area'].max():.2f} Å²\",\n",
        "            'Average hydrogen bonds': f\"{df['hydrogen_bonds'].mean():.2f}\",\n",
        "            'Max hydrogen bonds': f\"{df['hydrogen_bonds'].max():.0f}\",\n",
        "            'Average hydrophobic contacts': f\"{df['hydrophobic_contacts'].mean():.2f}\",\n",
        "            'Max hydrophobic contacts': f\"{df['hydrophobic_contacts'].max():.0f}\",\n",
        "            'Average salt bridges': f\"{df['salt_bridges'].mean():.2f}\",\n",
        "            'Max salt bridges': f\"{df['salt_bridges'].max():.0f}\"\n",
        "        }\n",
        "\n",
        "        # Find top design by BSA\n",
        "        top_bsa_idx = df['buried_surface_area'].idxmax()\n",
        "        top_bsa_design = df.loc[top_bsa_idx, 'design']\n",
        "        top_bsa_file = df.loc[top_bsa_idx, 'file_name']\n",
        "        stats['Top design by buried surface area'] = f\"Design {top_bsa_design} ({top_bsa_file})\"\n",
        "\n",
        "        # Create table\n",
        "        rows = len(stats) + 1  # +1 for header\n",
        "        cols = 2\n",
        "        table = summary_slide.shapes.add_table(rows, cols, left, top, width, height).table\n",
        "\n",
        "        # Header row\n",
        "        table.cell(0, 0).text = \"Metric\"\n",
        "        table.cell(0, 1).text = \"Value\"\n",
        "\n",
        "        # Add stats to table\n",
        "        for i, (metric, value) in enumerate(stats.items(), 1):\n",
        "            table.cell(i, 0).text = metric\n",
        "            table.cell(i, 1).text = str(value)\n",
        "\n",
        "        # Format header row\n",
        "        for cell in table.rows[0].cells:\n",
        "            cell.fill.solid()\n",
        "            cell.fill.fore_color.rgb = (200, 200, 200)\n",
        "\n",
        "        # Save PowerPoint\n",
        "        pptx_path = os.path.join(output_dir, f\"{output_basename}_{timestamp}_summary.pptx\")\n",
        "        prs.save(pptx_path)\n",
        "        print(f\"Saved PowerPoint summary to {pptx_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating PowerPoint summary: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ===============================\n",
        "# Main Processing Function\n",
        "# ===============================\n",
        "\n",
        "def process_multiple_pdb_files(pdb_directory: str, af2_scores_file: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Main processing function that:\n",
        "    1. Analyzes all PDB files in directory\n",
        "    2. Merges with AF2 scores if available\n",
        "    3. Generates visualizations and outputs\n",
        "    \"\"\"\n",
        "    if not os.path.exists(pdb_directory):\n",
        "        raise FileNotFoundError(f\"Directory not found: {pdb_directory}\")\n",
        "\n",
        "    # Get timestamp for file naming\n",
        "    timestamp = time.strftime(\"%y%m%d\")\n",
        "\n",
        "    # Initialize results\n",
        "    results = []\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    pdb_files = [f for f in os.listdir(pdb_directory) if f.endswith('.pdb')]\n",
        "\n",
        "    if not pdb_files:\n",
        "        print(f\"No PDB files found in {pdb_directory}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Processing {len(pdb_files)} PDB files...\")\n",
        "    total_files = len(pdb_files)\n",
        "\n",
        "    # Process each PDB file\n",
        "    for idx, file_name in enumerate(pdb_files, 1):\n",
        "        pdb_file = os.path.join(pdb_directory, file_name)\n",
        "        print(f\"Processing file {idx}/{total_files}: {file_name}\")\n",
        "\n",
        "        structure = safe_structure_load(parser, pdb_file)\n",
        "        if not structure:\n",
        "            continue\n",
        "\n",
        "        # Calculate structural parameters\n",
        "        buried_surface_area, chain_areas = calculate_buried_surface_area(pdb_file)\n",
        "        h_bonds = calculate_hydrogen_bonds(structure)\n",
        "        hydrophobic = calculate_hydrophobic_contacts(structure)\n",
        "        salt_bridges = calculate_salt_bridges(structure)\n",
        "\n",
        "        results.append({\n",
        "            'file_name': file_name,\n",
        "            'buried_surface_area': buried_surface_area,\n",
        "            'hydrogen_bonds': h_bonds,\n",
        "            'hydrophobic_contacts': hydrophobic,\n",
        "            'salt_bridges': salt_bridges,\n",
        "            'chain_areas': chain_areas\n",
        "        })\n",
        "\n",
        "    # Save structural analysis\n",
        "    output_basename = os.path.basename(pdb_directory)\n",
        "    structure_csv = os.path.join(pdb_directory, f\"{output_basename}_{timestamp}_structure.csv\")\n",
        "    structure_df = save_results_as_df(results, structure_csv)\n",
        "\n",
        "    # Check if we have valid data\n",
        "    if structure_df.empty:\n",
        "        print(\"Warning: No valid data extracted from analysis\")\n",
        "        return structure_df\n",
        "\n",
        "    # If AF2 scores exist, merge and create visualizations\n",
        "    if af2_scores_file and os.path.exists(af2_scores_file):\n",
        "        print(f\"Merging with AF2 scores from {af2_scores_file}\")\n",
        "        try:\n",
        "            final_df = merge_with_af2_scores(structure_df, af2_scores_file)\n",
        "\n",
        "            # Save combined analysis\n",
        "            combined_csv = os.path.join(pdb_directory, f\"{output_basename}_{timestamp}_combined.csv\")\n",
        "            final_df.to_csv(combined_csv, index=False)\n",
        "            print(f\"Saved combined results to {combined_csv}\")\n",
        "\n",
        "            # Create PowerPoint plots (original functionality)\n",
        "            # This has been modified for compatibility with the scenario where 'seq' is not available\n",
        "\n",
        "            return final_df\n",
        "        except Exception as e:\n",
        "            print(f\"Error during AF2 score merging: {str(e)}\")\n",
        "            print(\"Continuing with structure analysis only\")\n",
        "\n",
        "    # Generate simple visualizations for structural parameters only\n",
        "    try:\n",
        "        create_structure_visualization(structure_df, pdb_directory, timestamp)\n",
        "        create_pptx_summary(structure_df, pdb_directory, timestamp)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualizations: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return structure_df\n",
        "\n",
        "# ===============================\n",
        "# Main Execution\n",
        "# ===============================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set directory containing PDB files and AF2 scores\n",
        "    pdb_directory = '/content/drive/MyDrive/Evolving_hits_using_ProteinMPNN/20250303-3NOBEK/0_top_binders'  # Update this path\n",
        "    af2_scores_path = os.path.join(pdb_directory, 'af2_scores.csv')\n",
        "\n",
        "    # Check if AF2 scores exist\n",
        "    if not os.path.exists(af2_scores_path):\n",
        "        af2_scores_path = None\n",
        "        print(\"No AF2 scores file found - will generate structure analysis only\")\n",
        "\n",
        "    print(\"\\nStarting analysis...\")\n",
        "    print(f\"Processing PDB files from: {pdb_directory}\")\n",
        "\n",
        "    try:\n",
        "        results_df = process_multiple_pdb_files(pdb_directory, af2_scores_path)\n",
        "\n",
        "        if not results_df.empty:\n",
        "            print(\"\\nSummary of results:\")\n",
        "            print(f\"Total structures analyzed: {len(results_df)}\")\n",
        "            print(f\"Average buried surface area: {results_df['buried_surface_area'].mean():.2f} Å²\")\n",
        "            print(f\"Average hydrogen bonds: {results_df['hydrogen_bonds'].mean():.2f}\")\n",
        "            print(f\"Average hydrophobic contacts: {results_df['hydrophobic_contacts'].mean():.2f}\")\n",
        "            print(f\"Average salt bridges: {results_df['salt_bridges'].mean():.2f}\")\n",
        "\n",
        "            print(\"\\nTop structures by buried surface area:\")\n",
        "            top_bsa = results_df.nlargest(3, 'buried_surface_area')\n",
        "            for idx, row in top_bsa.iterrows():\n",
        "                print(f\"Design {row['design']} - {row['file_name']}: {row['buried_surface_area']:.2f} Å²\")\n",
        "\n",
        "            print(\"\\nTop structures by hydrogen bonds:\")\n",
        "            top_hbonds = results_df.nlargest(3, 'hydrogen_bonds')\n",
        "            for idx, row in top_hbonds.iterrows():\n",
        "                print(f\"Design {row['design']} - {row['file_name']}: {row['hydrogen_bonds']} bonds\")\n",
        "\n",
        "        print(\"\\nAnalysis completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during analysis: {str(e)}\")\n",
        "        traceback.print_exc()  # Print full stack trace for better debugging\n",
        "\n"
      ]
    }
  ]
}